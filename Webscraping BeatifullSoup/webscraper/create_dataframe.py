"""
create_dataframe.py

Este m칩dulo contiene funciones para construir y consolidar DataFrames con los enlaces de productos extra칤dos y sus metadatos.
Incluye utilidades para asociar los links a su contexto (pa칤s, nombre, paginaci칩n, etc.) y para procesar m칰ltiples URLs en paralelo.

Funciones:
- construir_dataframe_links: Construye un DataFrame con los links y metadatos asociados a una URL base.
- procesar_fila: Procesa una fila del DataFrame de URLs, extrayendo los links de productos.
- extraer_links_todas_las_urls: Extrae y consolida los links de productos de todas las URLs usando concurrencia.

Dependencias externas:
- Requiere funciones y patrones definidos en extraction.py y config.py.
- Utiliza pandas, concurrent.futures y tqdm para procesamiento y visualizaci칩n de progreso.
"""

from packages import List, pd
from .extraction import extraer_links_url
from .config import posibles_subcadenas
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

#==========================
#--- Construir_dataframe_links
#==========================

def construir_dataframe_links(df_urls: pd.DataFrame, url_base: str, lista_links: List[str]) -> pd.DataFrame:
    """
    Construye un DataFrame con metadatos asociados a una URL base y una lista de links de productos.

    Args:
        df_urls (pd.DataFrame): DataFrame que contiene las URLs base y sus metadatos.
        url_base (str): URL base usada para extraer productos.
        lista_links (List[str]): Lista de URLs de productos extra칤dos.

    Returns:
        pd.DataFrame: DataFrame con cada link asociado a su contexto (pa칤s, nombre, paginaci칩n, etc).
    """
    # Validar que la URL base est칠 presente
    coincidencias = df_urls[df_urls['url final'] == url_base]
    
    if coincidencias.empty:
        print(f"[丘멆잺] construir_dataframe_links: No se encontr칩 metadata para URL base: {url_base}")
        return pd.DataFrame()  # Devuelve DataFrame vac칤o si no hay coincidencias

    fila = coincidencias.iloc[0]

    # Crear el DataFrame final replicando los metadatos por cada link
    df_resultado = pd.DataFrame({
        'url final': lista_links,
        'Code Country': fila['Code Country'],
        'Country': fila['Country'],
        'Name': fila['Name'],
        'Information': fila['Information'],
        'Type Pagination': fila['Type Pagination'],
        'Note': fila['Note'],
        'Secuencia de paginacion': fila['Secuencia de paginacion']
    })

    print(f"construir_dataframe_links: Se cre칩 el DataFrame con {len(lista_links)} links para {fila['Country']} - {fila['Name']}")
    return df_resultado

#==========================
#--- Procesar_fila
#==========================
def procesar_fila(df_urls: pd.DataFrame, fila: pd.Series) -> pd.DataFrame:
    url_base = fila['url final']
    try:
        sec_pag = int(fila['Secuencia de paginacion'])
    except ValueError:
        print(f"[丘멆잺] Error convirtiendo secuencia de paginaci칩n a int: {fila['Secuencia de paginacion']}")
        sec_pag = 1  # Valor por defecto

    links = extraer_links_url(
        url_base=url_base,
        sec_pag=sec_pag,
        posibles_subcadenas=posibles_subcadenas,
        pausa=5  # Puedes ajustar esto
    )

    if links:
        return construir_dataframe_links(df_urls, url_base, links)
    else:
        return pd.DataFrame()  # Devuelve vac칤o si no se extrajo nada
    

#==========================
#--- Guardar_resultado_parcial
#==========================
import os
def guardar_resultado_parcial(df, ruta_csv="resultados_parciales.csv"):
    """
    Guarda el DataFrame recibido en un archivo CSV, agregando si ya existe.
    """
    if not df.empty:
        if os.path.exists(ruta_csv):
            df.to_csv(ruta_csv, mode='a', header=False, index=False)
        else:
            df.to_csv(ruta_csv, mode='w', header=True, index=False)
        print(f"[游쬫 Guardado parcial: {len(df)} filas en {ruta_csv}")


#==========================
#--- Extraer_links_todas_las_urls
#==========================
def extraer_links_todas_las_urls(df_urls: pd.DataFrame, max_workers: int = 50) -> pd.DataFrame:
    """
    Extrae links de productos de todas las URLs en el DataFrame.
    Utiliza m칰ltiples hilos para acelerar el proceso.
    Args:
        df_urls (pd.DataFrame): DataFrame que contiene las URLs base y sus metadatos.
        max_workers (int): N칰mero m치ximo de hilos a utilizar.
    Returns:
        pd.DataFrame: DataFrame consolidado con todos los links extra칤dos y sus metadatos.
    """
    resultados = []

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futuros = [
            executor.submit(procesar_fila, df_urls, fila)
            for _, fila in df_urls.iterrows()
        ]

        for futuro in tqdm(as_completed(futuros), total=len(futuros), desc="Procesando URLs"):
            resultado = futuro.result()
            if not resultado.empty:
                resultados.append(resultado)
                guardar_resultado_parcial(resultado)  # <-- Guarda cada resultado parcial

    if resultados:
        df_consolidado = pd.concat(resultados, ignore_index=True)
        print(f"[九] Se extrajeron {len(df_consolidado)} links en total.")
        return df_consolidado
    else:
        print("[丘멆잺] No se extrajo ning칰n link.")
        return pd.DataFrame()
    

